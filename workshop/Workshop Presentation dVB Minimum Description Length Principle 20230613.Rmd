---
title: Minimum Description Length Principle
subtitle: With An Application For Credit Risk Models
author: 
   - Paul van Leeuwen (paul.vanleeuwen@devolksbank.nl)
date: 16 May 2023
output:
  beamer_presentation:
    theme: Singapore
  ioslides_presentation: default
  slidy_presentation: default
bibliography: bibliography.bib
urlcolor: blue
header-includes:
  \usepackage{bm}
  \usepackage{tabu}
  \usepackage{amssymb}
  \usepackage{amsmath}
  \usepackage{colortbl}
  \usepackage{booktabs}
  \usepackage{array}
  \usepackage{natbib}
  \usepackage{mathtools}
  \usepackage{fancyvrb}
  \usepackage{fancyhdr}
  \usepackage{graphicx}
  \usepackage{verbatim}
  \usepackage{color}
  \usepackage{todo}
---

```{r setup, include = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
ixGraphicalDevices <- dev.list()['RStudioGD']
if (!is.null(ixGraphicalDevices)) {
  dev.off()
}
rm(list = ls())

library(tidyverse)
library(lubridate)
library(reshape2)
library(ggridges)
library(ggrepel)
library(maps)
library(kableExtra)
library(ggfortify)
library(latex2exp)
library(tidyquant)
library(glmnet)
theme_set(theme_bw(base_size = 18))  # pre-set the bw theme.
```



\tableofcontents

# Introduction

## Why this subject?

- Improve current approaches and modelling techniques.
- Stay in the forefront of statistical innovation.
- Connection with other realms of statistics.


# What is MDL?

## Introduction

- The Minimum Description Length (MDL) principle aims to describe the data and its description mechanism with the smallest possible information `length'.
- Applications (among else):
  - model selection (e.g. what order of the Markov model family do we want?);
  - deal with overfitting (e.g. how many explanatory variables to include);
  - exploratory data analysis (what prior knowledge can we confirm?).
- Close ties with frequentist statistics, Bayesian statistics, and machine learning.
- Why is MDL relatively unknown?
  - MDL is the intersection of advanced measure theory, information theory, and statistics.
  - For a decent introduction into MDL, see [@grunwald2007minimum].


## Example of dealing with overfitting

```{r}
set.seed(1)
dataObserved <- tibble(x = seq(-1, 1, by = 0.07), y = 5 + 2*x^2 + x^3 + rnorm( length(x), 0, 0.3 ) )
dataObserved <- 
  dataObserved %>% 
  mutate(
    linear = lm( y ~ poly(x, 1), dataObserved )$fitted, 
    quadratric = lm( y ~ poly(x, 2), dataObserved )$fitted, 
    `10th order polynomial` = lm( y ~ poly(x, 10), dataObserved )$fitted )
dataPlot <- 
  dataObserved %>% 
  pivot_longer(-c(x, y), names_to = 'model', values_to = 'fitted value') %>%
  mutate(model = factor(model, levels = c('linear', 'quadratric', '10th order polynomial')))
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    theme_minimal() + 
    ggtitle(TeX('What polynomial generated by this dataset?')) + 
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```


## Example of dealing with overfitting

```{r}
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    geom_line(data = dataPlot %>% filter(model == 'linear'), aes(x = x, y = `fitted value`), size = 2) +
    theme_minimal() + 
    ggtitle(TeX('linear fit $\\hat{y} = \\beta_0 + \\beta_1 x$')) +
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```


## Example of dealing with overfitting

```{r}
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    geom_line(data = dataPlot %>% filter(model == 'quadratric'), aes(x = x, y = `fitted value`), size = 2) +
    theme_minimal() + 
    ggtitle(TeX('quadratric fit $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$')) +
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```


## Example of dealing with overfitting

```{r}
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    geom_line(data = dataPlot %>% filter(model == '10th order polynomial'), aes(x = x, y = `fitted value`), size = 2) +
    theme_minimal() + 
    ggtitle(TeX('$10^\\textrm{th}$ order polynomial fit $\\hat{y} = \\beta_0 + \\beta_1 x + \\ldots + \\beta_{10} x^{10}$')) +
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```


## Example of dealing with overfitting

```{r}
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    geom_line(data = dataPlot, aes(x = x, y = `fitted value`, colour = model), size = 2) + 
    facet_wrap(model ~ .) +
    theme_minimal() + 
    ggtitle(TeX('data generated by $x^3 + 2 x^2 + 5 + \\epsilon$, $\\epsilon \\sim N(0, 1)$')) + 
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```



## How does MDL work?

- Patterns or regularities in the data can be described with less information `length' compared to the data alone.
- less information `length' = compression
- Choose the model that gives the shortest description of the data.
- Note that MDL is an approach, not an algorithm.
  - The modeller has to make choices to implement the MDL principle.


# Approach

## From Data to Model Selection

- From data to code.
- From code to code length.
- From code length to probabilities.
- From probabilities to model selection.


## From Data to Code

- Examples:
  - `Hello, world` could map to 0.
  - `aabbaccdaaadd` could map to 110100.
- In general, a description method maps a sequence of symbols to a binary sequence.
  - The coding alphabet $\mathbb{B}$ can be binary ($\mathbb{B} = \{0, 1\}$), the Western alphabet ($\mathbb{B} = \{\textrm{a}, \textrm{b}, \ldots, \textrm{z} \}$), etc.
- Mathematically: a dataset $D = (x_1, \ldots, x_n)$ with $x_i \in \mathbb{B}$ from a sample space $\mathcal{X}^n$ is mapped to $\{0, 1\}^m$ by $C \colon \mathcal{X}^n \mapsto \{0, 1\}^m$.
  - In the first examples above, we could have $\mathcal{X}^n = \{x_1\} =\{\mathrm{'Hello, world'}\}$ and $C(x_1) = 0$.
- We demand the mapping $C$ to be \textit{uniquely} decodable.
  - No multiple interpretations allowed.


## From Data to Code

- Suppose we would like to encode a binary data sequence of length 2.
- We are not sure what outcome we observe.
- Let $X_i \in \{0, 1\}$ be the random variable at position $i = 1, 2$.
  - The complete data sequence becomes $X_1 X_2 \in \{00, 10, 01, 11\}$.
- Every sequence in $\{00, 10, 01, 11\}$ is assigned a code.
- In general, for the binary alphabet, for $n$ positions there are $2^n$ possible data sequences.
  - For example, when $n = 3$ we have the data sequences $\{000, 100, 010, \ldots, 111\}$.
  - Without loss of generality, every non-binary alphabet can be mapped to the binary alphabet $\{0, 1\}$.


## From Code to Code Length

- Given a data sequence $x_i$ and its corresponding code $C(x_i)$, then we are interested in the code length $L(x_i)$ with $L \colon \mathcal{X}^n \mapsto \mathbb{R}_+$.
- For example, to map an integer from $\{ 1, \ldots, n \}$ in a uniform way, we need $\left \lceil \log_2(n) \right \rceil$ bits.
  - Note that $n$ has to be known in advance.
  - For example, take $n = 64$. Then we have 64 binary data sequences of length $\left \lceil \log_2(64) \right \rceil = 6$.
  - Or take $n = 10$. Then we need $\left \lceil \log_2(10) \right \rceil = 4$ bits.
    - Note that $2^4 = 16$ data sequences are possible while we only use 10 of them.
- What coding scheme results in the smallest number of \textit{expected} bits?


## From Code to Code Length

- Recall the data sequences $\{00, 10, 01, 11\}$.
- Uniform method: the data sequence is the code.
  - $C(00) = 00$, $C(10) = 10$, $C(01) = 01$, and $C(11) = 11$.
  - Expected number of bits: 2.
- In general we can do better!
  - That is, $\mathbb{P}[X_i = 1] \neq \frac{1}{2}$ for at least one $i \in \{1, \ldots, n\}$.


## From Code Length to Probabilities

- Suppose $\mathbb{P}[X_i = 1] = \frac{1}{4}$.
- We reserve code 0 for $X_1 X_2 = 00$ so $C(00) = 0$, $C(10) = 100$, $C(01) = 110$, and $C(11) = 1110$.
- Then the expected number of bits is
\[
1 \cdot \tfrac{3}{4}^2 + 3 \cdot \tfrac{3}{4} \cdot \tfrac{1}{4} + 3 \cdot \tfrac{1}{4} \cdot \tfrac{3}{4} + 4 \cdot \tfrac{1}{4}^2 = 1.9375 < 2
\]
- This the Shannon-Fano coding scheme.
  - More general, reserve $\lceil - \log_2(p_i) \rceil$ bits for probability $p_i$ corresponding to data sequence $i$.
  - The Huffman code is optimal and improves on the Shannon-Fano code.
- Main message: 

\fbox{\begin{minipage}{\textwidth}
\center
higher probability = smaller code length = less bits required
\end{minipage}}


## From Probabilities to Model Selection

- To describe any dataset we need $L(D, H)$ bits.
  - Both the description method $H$ and the data $D$ require bits.
- The MDL principle employed for model selection is to minimise the sum of 
  - the number of bits to encode the description mechanism $L(H)$ and
  - the number of bits to encode, with the description mechanism $H$, the data observed $L(D | H)$.
- Information `length' is this sum $L(H) + L(D | H)$.
  - $L(H)$ is the \textit{model complexity}, $L(D | H)$ is the \textit{fit of the data}.
- The MDL principle is to choose the model as to minimise this sum.
- Main message: 

\fbox{\begin{minipage}{\textwidth}
\center
smaller description length = better model selection
\end{minipage}}


## Example of dealing with overfitting (continued)

```{r}
dataPlot <- 
    tribble(
        ~model, ~`model complexity`, ~residuals,
        'linear', 2, 10,
        'quadratric', 3, 5,
        '10-th order polynomial', 11, 3
    ) %>% 
  mutate(
    model = factor(model, levels = c('linear', 'quadratric', '10-th order polynomial')),
    `description length` = `model complexity` + residuals)
print(
  dataPlot %>% 
    pivot_longer(-model) %>% 
    mutate(name = factor(name, levels = c('model complexity', 'residuals', 'description length'))) %>%
    ggplot(aes(x = model, y = value, fill = name)) + geom_col(position = 'dodge') + theme(legend.title=element_blank())
)
```


## Bibliography