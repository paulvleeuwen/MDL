---
title: Minimum Description Length Principle
subtitle: With An Application For Credit Risk Models
author: 
   - Paul van Leeuwen (paul.vanleeuwen@devolksbank.nl)
date: 7 February 2023
output:
  beamer_presentation:
    theme: Singapore
  ioslides_presentation: default
  slidy_presentation: default
bibliography: bibliography.bib
urlcolor: blue
header-includes:
  \usepackage{bm}
  \usepackage{tabu}
  \usepackage{amssymb}
  \usepackage{amsmath}
  \usepackage{colortbl}
  \usepackage{booktabs}
  \usepackage{array}
  \usepackage{natbib}
  \usepackage{mathtools}
  \usepackage{fancyvrb}
  \usepackage{fancyhdr}
  \usepackage{graphicx}
  \usepackage{verbatim}
  \usepackage{color}
  \usepackage{todo}
---

```{r setup, include = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
ixGraphicalDevices <- dev.list()['RStudioGD']
if (!is.null(ixGraphicalDevices)) {
  dev.off()
}
rm(list = ls())

library(tidyverse)
library(lubridate)
library(reshape2)
library(ggridges)
library(ggrepel)
library(maps)
library(kableExtra)
library(ggfortify)
library(latex2exp)
library(tidyquant)
library(glmnet)
theme_set(theme_bw(base_size = 18))  # pre-set the bw theme.
```



\tableofcontents

# Introduction

## The Speaker

- Paul van Leeuwen, employee of KRT RDS 5.0 as of September 2022.
- Up to 2017 part of the Modelling team at dVB.
- After that as Model Validator at Achmea and Lead Data Scientist at Wageningen University & Research.
- Now self-employed and currently working in the financial sector and providing `R` workshops.


## Why this subject?

- Improve current approaches and modelling techniques.
- Stay in the forefront of statistical innovation.
- Connection with other realms of statistics.


# What is MDL?

## Introduction

- The Minimum Description Length (MDL) principle aims to describe the data and its description mechanism with the smallest possible information `length'.
- Applications (among else):
  - model selection (e.g. what order of the Markov model family do we want?);
  - deal with overfitting (e.g. how many explanatory variables to include);
  - exploratory data analysis (what prior knowledge can we confirm?).
- Close ties with frequentist statistics, Bayesian statistics, and machine learning.
- Why is MDL relatively unknown?
  - MDL is the intersection of advanced measure theory, information theory, and statistics.
  - For a decent introduction into MDL, see [@grunwald2007minimum].


## Example of dealing with overfitting

```{r}
set.seed(1)
dataObserved <- tibble(x = seq(-1, 1, by = 0.07), y = 5 + 2*x^2 + x^3 + rnorm( length(x), 0, 0.3 ) )
dataObserved <- 
  dataObserved %>% 
  mutate(
    linear = lm( y ~ poly(x, 1), dataObserved )$fitted, 
    quadratric = lm( y ~ poly(x, 2), dataObserved )$fitted, 
    `10th order polynomial` = lm( y ~ poly(x, 10), dataObserved )$fitted )
dataPlot <- 
  dataObserved %>% 
  pivot_longer(-c(x, y), names_to = 'model', values_to = 'fitted value') %>%
  mutate(model = factor(model, levels = c('linear', 'quadratric', '10th order polynomial')))
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    theme_minimal() + 
    ggtitle(TeX('What polynomial generated by this dataset?')) + 
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```


## Example of dealing with overfitting

```{r}
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    geom_line(data = dataPlot %>% filter(model == 'linear'), aes(x = x, y = `fitted value`), size = 2) +
    theme_minimal() + 
    ggtitle(TeX('linear fit $\\hat{y} = \\beta_0 + \\beta_1 x$')) +
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```


## Example of dealing with overfitting

```{r}
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    geom_line(data = dataPlot %>% filter(model == 'quadratric'), aes(x = x, y = `fitted value`), size = 2) +
    theme_minimal() + 
    ggtitle(TeX('quadratric fit $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$')) +
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```


## Example of dealing with overfitting

```{r}
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    geom_line(data = dataPlot %>% filter(model == '10th order polynomial'), aes(x = x, y = `fitted value`), size = 2) +
    theme_minimal() + 
    ggtitle(TeX('$10^\\textrm{th}$ order polynomial fit $\\hat{y} = \\beta_0 + \\beta_1 x + \\ldots + \\beta_{10} x^{10}$')) +
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```


## Example of dealing with overfitting

```{r}
print(
  ggplot() + 
    geom_point(data = dataPlot, aes(x = x, y = y)) + 
    geom_line(data = dataPlot, aes(x = x, y = `fitted value`, colour = model), size = 2) + 
    facet_wrap(model ~ .) +
    theme_minimal() + 
    ggtitle(TeX('data generated by $x^3 + 2 x^2 + 5 + \\epsilon$, $\\epsilon \\sim N(0, 1)$')) + 
    scale_y_discrete(labels = NULL, breaks = NULL) + 
    scale_x_discrete(labels = NULL, breaks = NULL) + 
    ylab('y')
)
```



## How does MDL work?

- Patterns or regularities in the data can be described with less information `length' compared to the data alone.
- less information `length' = compression
- Choose the model that gives the shortest description of the data.
- Note that MDL is an approach, not an algorithm.
  - The modeller has to make choices to implement the MDL principle.


## Example of MDL

```{r}
sequence_1 <- paste( rep(1000, 250), collapse = '' )
sequence_2 <- paste( sample(0:1, 1e3, replace = TRUE), collapse = '' )
sequence_3 <- paste( ifelse( sample(0:4, 100, replace = TRUE) == 1, 1, 0 ), collapse = '' )
textSequence_1 <- paste( c( substr(sequence_1, 1, 43), substr( sequence_1, nchar(sequence_1) - 3, nchar(sequence_1) ) ), collapse = ' ... ' )
textSequence_2 <- paste( c( substr(sequence_2, 1, 43), substr( sequence_2, nchar(sequence_2) - 3, nchar(sequence_2) ) ), collapse = ' ... ' )
textSequence_3 <- paste( c( substr(sequence_3, 1, 43), substr( sequence_3, nchar(sequence_3) - 3, nchar(sequence_3) ) ), collapse = ' ... ' )
```
- Consider three data-generating processes (dgp's) that generate each a binary sequence of length 1000:
  \begin{enumerate}
    \item `r textSequence_1`
    \item `r textSequence_2`
    \item `r textSequence_3`
  \end{enumerate}
\pause 
- All sequences written out take 1000 bits to be reproduced.
- However, because of regularities present in the dgp's, we require less bits to reproduce the same sequences.
- Question: how many bits of Matlab code does each sequence take to be \textit{exactly} reproduced?


## Example of MDL

- The sequences as before: 
  1. `r textSequence_1`:
     \pause
     repetition of `[1 0 0 0]` 250 times.\
     Matlab-code:\
     `textDgp_1 = 'repmat([1 0 0 0],1,250)'`\
     `whos_textDgp_1 = whos('textDgp_1')`\
     `whos_textDgp_1.bytes`\
     yields 23 bytes = 184 bits, a compression ratio of $1 - \tfrac{184}{1000} = 81.6\%$!
     \pause
    
  2. `r textSequence_2`: 
     \pause
     a coin toss with heads (0) or tails (1); no compression possible because of the randomness involved.
  
  3. `r textSequence_3`:
     \pause
     a roll with a four-sided die with outcomes 2, 3, 4 assigned to 0 and outcome 1 assigned to 1; no compression possible because of the randomness involved.


## The Principle of MDL

- The more randomness involved, the less data compression is possible.
- When the restriction of exact reproduction is alleviated we may obtain some data compression.
- Dgp 2 (the coin toss) implies no data compression.
  - As the data sequence is completely random.
- Dgp 3 (the 4-sided die) implies some possible data compression.
  - Although exact reproduction is not possible the set of this type of data sequences requires around `r round(log2(1000) - 250 * log2(.25) - 750 * log2(.75))` bits.
- Most datasets are almost incompressible.
  - Only a small fraction can be significantly compressed.
  

## Description Methods

- A description method in conjunction with a coding scheme maps an object to a number of bits.
  - The coding alphabet $\mathbb{B}$ can be binary ($\mathbb{B} = \{0, 1\}$), the Western alphabet ($\mathbb{B} = \{\textrm{a}, \textrm{b}, \ldots, \textrm{z} \}$), etc.
  - The mapping is a one-many relation and many-one.
  - Mathematically: a dataset $D = (x_1, \ldots, x_n)$ with $x_i \in \mathbb{B}$ from a sample space $\mathcal{X}^n$ is mapped to $\mathbb{R}_+$ by $L \colon \mathcal{X}^n \mapsto \mathbb{R}_+$.
- Any alphabet is fine.
- For example, to map an integer from $\{ 1, \ldots, n \}$ in a uniform way, we need $\log_2 n$ bits.
  - Note that $n$ has to be known in advance.
  - For example, take $n = 64$. Then we have 64 binary data sequences of length $\log_2(64) = 6$.


## The Principle of MDL (continued)

- To describe any dataset we need $L(D, H)$ bits.
  - Both the description method $H$ and the data $D$ require storage space.
- The MDL principle employed for model selection is to minimise the sum of 
  - the number of bits to encode the description mechanism $L(H)$ and
  - the number of bits to encode, with the description mechanism $H$, the data observed $L(D | H)$.
- Information `length' is this sum $L(H) + L(D | H)$.
- The former is the \textit{model complexity}, the latter the \textit{fit of the data}.
- The MDL principle is to choose the model specification as to minimise this sum.


## The Principle of MDL (continued)

- More mathematically, given a set of candidate models $\mathcal{H} = \{ \mathcal{H}_1, \mathcal{H}_2, \ldots \}$, select the optimal model $\mathcal{H}^\star \in \mathcal{H}$ as
\[
\mathcal{H}^\star := \operatorname*{arg\,min}_{H \in \mathcal{H}} L(D, H) = \operatorname*{arg\,min}_{H \in \mathcal{H}} \{ L(H) + L(D | H) \}
\]
- Note the resemblance with penalised model fitting, such as LASSO:
  - with LASSO we apply cross-validation to minimise
  \[
  \operatorname*{arg\,min}_{\bm{\beta}} \left\{ \frac{1}{n} \| \bm{y} - \bm{X} \bm{\beta} \|_2^2 + \lambda \| \bm{\beta} \|_1 \right\}
  \]
  with the data part $\frac{1}{n} \| \bm{y} - \bm{X} \bm{\beta} \|_2^2$ ($L(D | H)$ in MDL) and $\lambda \| \bm{\beta} \|_1$ the model complexity part ($L(H)$ in MDL).


## Example of LASSO

```{r}
set.seed(1010)
n = 1000
p = 100
nzc = trunc(p/10)
x = matrix(rnorm(n * p), n, p)
beta = rnorm(nzc)
fx = x[, seq(nzc)] %*% beta
eps = rnorm(n) * 5
y = drop(fx + eps)
px = exp(fx)
px = px/(1 + px)
ly = rbinom(n = length(px), prob = px, size = 1)
set.seed(1011)
cvob1 = cv.glmnet(x, y)
tibble(lambda = cvob1$lambda, `mean-squared error out-of-sample` = cvob1$cvm) %>% ggplot(aes(x = log(lambda), y = `mean-squared error out-of-sample`)) + geom_line() + geom_point()
```


## Example of dealing with overfitting (continued)

```{r}
dataPlot <- 
    tribble(
        ~model, ~`model complexity`, ~residuals,
        'linear', 2, 10,
        'quadratric', 3, 5,
        '10-th order polynomial', 11, 3
    ) %>% 
  mutate(
    model = factor(model, levels = c('linear', 'quadratric', '10-th order polynomial')),
    `description length` = `model complexity` + residuals)
print(
  dataPlot %>% 
    pivot_longer(-model) %>% 
    mutate(name = factor(name, levels = c('model complexity', 'residuals', 'description length'))) %>%
    ggplot(aes(x = model, y = value, fill = name)) + geom_col(position = 'dodge') + theme(legend.title=element_blank())
)
```


## Questions to be answered

- How does MDL work in practice?
- What is the best approach: frequentist statistics, Bayesian statistics, machine learning, or MDL?
- The coding system to put down the description mechanism should not matter. How can we choose a universal programming language?
  - For example, whether we use `Matlab`, `R` or whatever programming language should not matter for the number of bits at use.
- What are good encoding systems?
- What patterns are generalisable and what not?
- How to incorporate prior knowledge?



# MDL in Practice

## Dow Jones Industrial Average

- How can we employ MDL to compress a dataset?
  - Example taken from [@hansen2001model].
- Take the log daily return $R_t$ and the volatility $V_t$ of the Dow Jones Industrial Average (DJIA).
  - $t$ runs from $t_0$ = July 1962 until $T$ = June 1988, i.e. 6,430 trading days.
  - $R_t = P_t - P_{t-1}$ with $P_t$ the logarithm of the DJIA at day $t$.
  - $V_t = 0.9 V_{t-1} + 0.1R_t^2$ and $V_0$ the variance of the series $P_t$.
- $R_t$ has a corresponding indicator: 1 (0) when $R_t > R_{t-1}$ ($R_t < R_{t-1}$).
  - Analogously for $V_t$.
  - Two binary strings of length 6,430 - 1 = 6,429.
  - $R_t$ has 3,181 (49.49%) ups.
  - $V_t$ has 2,023 or (31.47%) ups.


## Dow Jones Industrial Average

```{r}
fileNameDji <- 'DJI.RData'
if(file.exists(fileNameDji)){
  load(fileNameDji)
}else{
  x <- getSymbols("DJI", from = '1950-01-01', to = "2018-03-01", warnings = FALSE, auto.assign = TRUE, return.class = 'data.frame')
  save(DJI, file = fileNameDji)
}
DJI %>% tibble() %>% mutate(`trading date` = as.Date(rownames(DJI)), closing = DJI.Close) %>% ggplot(aes(x = `trading date`, y = log(closing))) + geom_point(size = 0.5) + labs(title = 'logarithm of the closing of the Dow Jones Industrial Average')
```


## Dow Jones Industrial Average

- Without data compression we require 6,429 bits per series.
- Using MDL we save 10\% on the volatility series $V_t$ while gaining nothing on $P_t$.
- $n$ is known in advance so costs us $\lceil \log_2 n \rceil$ bits.
- Model the up or down indicator as a Bernoulli probability distribution with probability $p$ on success.
  - Maximum likelihood yields $\hat{p} = k \slash n$ with $k$ the number of successes.
- It takes us $-\log_2(k \slash n)$ bits to model a success and $-\log_2(1 - k \slash n)$ bits to model a failure.
- In total we need, ignoring rounding errors,
\[
\log_2(n) + k \left[-\log_2 \left( \frac{k}{n} \right) \right] + (n - k) \left[-\log_2 \left( 1 - \frac{k}{n} \right) \right]
\]


## Dow Jones Industrial Average

- With $n = 6,429$ and
  - for $R_t$ we have $k = 3181$ and $\hat{p} = \frac{k}{n} = \frac{3181}{6429} = 0.49$ we need
  \[
  \log_2(6429) + 3181 [-\log_2 0.49] + 3248 [-\log_2 0.51] = 6442 \textrm{ bits}
  \]
  - for $V_t$ we have $k = 2023$ and $\hat{p} = \frac{k}{n} = \frac{2023}{6429} = 0.31$ we need
  \[
  \log_2(6429) + 2023 [-\log_2 0.31] + 4406 [-\log_2 0.69] = 5789 \textrm{ bits}
  \]
- In conclusion, for $R_t$ we have gained nothing, even lost some bits, but the improvement for $V_t$ is substantial.
  - The savings in storage increases as well as $n$ increases or $k \slash n$ approaches 0 or 1.
- In general, data compression leads to good model performance.
  - Note that data compression is not the main goal.
  

# Approaches compared

## How do the approaches compare: Frequentist

- Also known as orthodox or non-Bayesian.
- Main goal: models should be consistent.
  - At least asymptotically: as $n \rightarrow \infty$ the estimated model coefficients should converge to the `true' model parameters.
  - In the example above, one would expect $\bm{ \hat{\beta} } = (5 \, 0 \, 2 \, 1)$ for $n \rightarrow \infty$.
- When the model assumptions are fulfilled, this approach yields the best possible predictions.
- Pros: widely known and applicable, easy calculation, models are good with infinite amount of data.
- Cons: assumptions are not realistic (e.g. almost no residual follows a normal probability distribution).


## How do the approaches compare: Bayesian

- Main goal: incorporate prior knowledge by assigning a prior probability distribution.
  - Once the prior probability distribution is assigned, the posterior probability distribution can be derived / calculated.
  - Frequentist models consider the data random and the parameters fixed.
  - Bayesian models consider the data and the parameters random.
  - In practice, asymptotically frequentist and Bayesian models converge.
- Pros: mathematically elegant and precise, encompasses the frequentist approach.
- Cons: hard to come up with a useful prior probability distribution.


## How do the approaches compare: Machine Learning

- Main goal: find and fit a model that is able to generalise from train to test data.
  - No assumptions about the data generating process.
- Pros: no assumptions required, good model performance.
- Cons: hard to explain predictions, hard to study the data generating process.


## How do the approaches compare: MDL

- Main goal: infer useful information from the data and use that to achieve good data compression.
  - Good data compression implies good learning.
  - No assumptions about the data generating process required but are allowed.
- Pros: no assumptions required, has best of all worlds (Bayesian and Machine Learning): employ prior knowledge when viable, good statistical properties, and decent performance.
- Cons: relatively new in literature, intersection of state-of-the-art knowledge of information theory, measure theory and statistics.


# Discussion

## Application to Expected Credit Loss

- Suppose we apply the MDL principle on the calculation of the Expected Credit Loss.
  - No coding scheme discussed here, just the principle.
- Discussion:
\begin{center}
\textit{take a moving average of the realised credit losses to calculate the Expected Credit Loss}
\end{center}
- Pros:
  - Occam's Razor: simpler is better.
  - Saves us a complete IFRS team to perform tedious calculations.
- Cons:
  - No new developments (e.g. Corona) taken into consideration. Can we with our current models?
  - No intermediate analyses available (e.g. use PD to rank customers in need).



# Conclusion

## Summary

- MDL is a guidance for model selection, an instrument against overfitting and exploratory data analysis.
- Sound statistics and decent model performance could make MDL a challenger for other statistical approaches.
- Much more to be explored.


## What's next?

- A workshop to take a deepdive into the applications of MDL, backed up by statistical theory.
- Take a look at the open questions raised in this presentation.
  - The coding system to put down the description mechanism should not matter. How can we choose a universal programming language?
  - What are good encoding systems?
  - What patterns are generalisable and what not?
  - How to incorporate prior knowledge?
- Dedicated application to IFRS and Regulatory Capital calculations.
- Please let Eelko (eelko.ubels@devolksbank.nl) or me (paul.vanleeuwen@devolksbank.nl) know whether you would like to join!


## Questions

\begin{center}
\LARGE ?
\end{center}



# Appendix

## Kolmogorov complexity

- Any application of MDL should not be affected by the programming language of choice.
- Andrey Kolmogorov postulated the invariance theorem:
\begin{center}
\textit{given any description of an object in a description language $A$, said description may be used in the optimal description language $B$ with a constant overhead}
\end{center}
- The description follows two steps:
  1. describe optimal language $B$ in language $A$;
  2. describe object in optimal language $B$ via language $A$.
- Comparable with a user-friendly programming language $A$ (e.g. Matlab, R, Python) and whose code is compiled in an efficient programming language $B$ (e.g. C++, C, FORTRAN).
- Part 1. is overhead independent of the object to be described, hence as the object size grows the overhead becomes small.


## Kolmogorov complexity (example)

- Suppose we would like to describe the binary data sequence `1 0 0 0` in C via Matlab. How many bits does it take?
- First translate any code from C to Matlab needed to describe `1 0 0 0`.
  - E.g. C has a certain mapping procedure from the data type bit to its memory.
- The corresponding number of bits of the Matlab-code is $L_{ \textrm{C} \, \mapsto \, \textrm{M} }$, e.g. 10k bits.
- To describe $D$ = `1 0 0 0` in C via Matlab takes $L_{ \textrm{C} \, \mapsto \, \textrm{M} } + L_{ \textrm{M} }(D)$ bits.
- But $L_{ \textrm{C} \, \mapsto \, \textrm{M} }$ is independent of the object to be described.
  - Whether we put in the works of Shakespeare or `Hello, World`, $L_{ \textrm{C} \, \mapsto \, \textrm{M} }$ remains the same.


## Bibliography